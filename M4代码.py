#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import json
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from datetime import datetime, timezone, timedelta
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
# æ–°å¢ï¼šå¯¼å…¥å‡†ç¡®ç‡è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import accuracy_score

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆä»…éœ€ä¿®æ”¹è¿™1ä¸ªè·¯å¾„ï¼‰ =====================
CORE_FOLDER = "/Users/shiyiwen/Desktop/ç§‘ç ”/æ•°æ®æå–ä»£ç /Kickstarter_2025-12-18T03_20_24_296Z"

# å®šä¹‰éœ€è¦æ£€æŸ¥ç¼ºå¤±çš„æ ¸å¿ƒå­—æ®µï¼ˆå¯æ ¹æ®å®é™…éœ€æ±‚å¢å‡ï¼‰
CHECK_MISSING_FIELDS = {
    "deadline": "æˆªæ­¢æ—¶é—´",
    "launched_at": "å‘å¸ƒæ—¶é—´",
    "goal": "ç­¹æ¬¾ç›®æ ‡",
    "fx_rate": "æ±‡ç‡",
    "country": "å›½å®¶",
    "category": "åˆ†ç±»ä¿¡æ¯",
    "name": "é¡¹ç›®åç§°",
    "id": "é¡¹ç›®ID"
}

# ç‰¹å¾åç§°æ˜ å°„ï¼ˆæ”¹ä¸ºè‹±æ–‡ï¼Œé€‚é…å›¾è¡¨ï¼‰
FEATURE_NAME_MAP = {
    "duration_days": "Funding Duration (days)",
    "cat_enc": "Project Category",
    "country_enc": "Country",
    "goal_usd": "Funding Goal (USD)",
    "fund_sim": "Goal Reasonableness",
    "time_ratio": "Duration Ratio (vs 7 days)"
}

# ===================== åˆå§‹åŒ–Matplotlib =====================
mpl.rcParams.update(mpl.rcParamsDefault)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False 
# ===================== æ ¸å¿ƒç±»ï¼šå…¨é‡CSVè®­ç»ƒ + å…¨é‡CSVé¢„æµ‹ =====================
class KSFullCSVProcessor:
    def __init__(self):
        # 1. è‡ªåŠ¨åˆ›å»ºæ¨¡å‹å­æ–‡ä»¶å¤¹ï¼Œç»Ÿä¸€å­˜æ”¾æ¨¡å‹æ–‡ä»¶
        self.model_dir = os.path.join(CORE_FOLDER, "model_output")
        os.makedirs(self.model_dir, exist_ok=True)
        
        # 2. è‡ªåŠ¨åˆ›å»ºé¢„æµ‹ç»“æœå­æ–‡ä»¶å¤¹ï¼Œå­˜æ”¾å•ä¸ªæ–‡ä»¶çš„é¢„æµ‹ç»“æœ
        self.pred_result_dir = os.path.join(CORE_FOLDER, "å•ä¸ªæ–‡ä»¶é¢„æµ‹ç»“æœ")
        os.makedirs(self.pred_result_dir, exist_ok=True)
        
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„æ”¹ä¸ºå­æ–‡ä»¶å¤¹å†…
        self.model_files = {
            "model": os.path.join(self.model_dir, "ks_model.pkl"),
            "scaler": os.path.join(self.model_dir, "ks_scaler.pkl"),
            "feats": os.path.join(self.model_dir, "ks_features.json"),
            "le_cat": os.path.join(self.model_dir, "ks_le_cat.pkl"),
            "le_country": os.path.join(self.model_dir, "ks_le_country.pkl"),
            "goal_median": os.path.join(self.model_dir, "ks_goal_median.json"),
            "feature_importance": os.path.join(self.model_dir, "feature_importance.json"),
            # æ–°å¢ï¼šä¿å­˜æ¨¡å‹å‡†ç¡®ç‡çš„æ–‡ä»¶è·¯å¾„
            "model_accuracy": os.path.join(self.model_dir, "model_accuracy.json")  
        }

    # -------------------------- è·å–å®æ—¶åŒ—äº¬æ—¶é—´æ—¶é—´æˆ³ --------------------------
    def _get_beijing_time_timestamp(self):
        """è·å–å®æ—¶åŒ—äº¬æ—¶é—´çš„ç§’çº§æ—¶é—´æˆ³ï¼ˆUTC+8ï¼‰"""
        beijing_tz = timezone(timedelta(hours=8))
        now = datetime.now(beijing_tz)
        return int(now.timestamp())

    # -------------------------- ç²¾å‡†è¯†åˆ«ç¼ºå¤±å­—æ®µ --------------------------
    def _get_missing_fields_detail(self, row):
        """
        è¯†åˆ«å•æ¡æ•°æ®ä¸­ç¼ºå¤±çš„å…·ä½“å­—æ®µï¼Œè¿”å›å¯è¯»çš„ç¼ºå¤±æè¿°
        :param row: DataFrameçš„å•è¡Œæ•°æ®
        :return: ç¼ºå¤±å­—æ®µæè¿°ï¼ˆå¦‚"ç¼ºå¤±æˆªæ­¢æ—¶é—´ã€ç¼ºå¤±å›½å®¶"æˆ–"æ— ç¼ºå¤±"ï¼‰
        """
        missing_fields = []
        for field_code, field_name in CHECK_MISSING_FIELDS.items():            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨ç¼ºå¤±ï¼ˆNaN/ç©ºå­—ç¬¦ä¸²/0ï¼ˆä»…é’ˆå¯¹éæ•°å€¼å­—æ®µï¼‰ï¼‰
            if pd.isna(row[field_code]):
                missing_fields.append(field_name)
            elif isinstance(row[field_code], str) and row[field_code].strip() == "":
                missing_fields.append(field_name)
            # å¯¹æ•°å€¼å‹å…³é”®å­—æ®µï¼ˆå¦‚idã€goalï¼‰ï¼Œ0ä¹Ÿè§†ä¸ºç¼ºå¤±
            elif field_code in ["id", "goal"] and row[field_code] == 0:
                missing_fields.append(field_name)
        
        if not missing_fields:
            return "æ— ç¼ºå¤±"
        else:
            return "ã€".join([f"ç¼ºå¤±{field}" for field in missing_fields])

    # -------------------------- åŸæœ‰å·¥å…·å‡½æ•° --------------------------
    def _safe_ts_convert(self, ts):
        if pd.isna(ts) or str(ts).lower() in ["nan", "", "None"]:
            return datetime(2020, 1, 1)
        try:
            ts_int = int(float(ts))
            ts_int = ts_int // 1000 if ts_int > 1e12 else ts_int
            return datetime.fromtimestamp(ts_int)
        except:
            return datetime(2020, 1, 1)

    def _safe_parse_category(self, cat_str):
        if pd.isna(cat_str) or str(cat_str).lower() in ["nan", "", "None"]:
            return "OTHER"
        try:
            cat_dict = json.loads(cat_str.replace("'", '"').strip())
            return cat_dict.get("parent_name", cat_dict.get("name", "OTHER")).strip()
        except:
            return "OTHER"    # -------------------------- æå–å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆè‹±æ–‡å›¾è¡¨ï¼‰ --------------------------
    def _extract_feature_importance(self, model, feature_cols):
        """
        ä»éšæœºæ£®æ—æ¨¡å‹ä¸­æå–ç‰¹å¾é‡è¦æ€§ï¼Œä¿å­˜å¹¶è¿”å›å¯è§†åŒ–çš„ç»“æœï¼ˆè‹±æ–‡å›¾è¡¨ï¼‰
        :param model: è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹
        :param feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
        :return: ç‰¹å¾é‡è¦æ€§å­—å…¸ï¼ˆè‹±æ–‡åç§°: é‡è¦æ€§å€¼ï¼‰
        """
        # è·å–ç‰¹å¾é‡è¦æ€§
        importances = model.feature_importances_
        # æ„å»ºç‰¹å¾é‡è¦æ€§å­—å…¸ï¼ˆä»£ç ç‰¹å¾å â†’ è‹±æ–‡ä¸šåŠ¡åç§° â†’ é‡è¦æ€§å€¼ï¼‰
        feat_imp_dict = {}
        for col, imp in zip(feature_cols, importances):
            feat_imp_dict[FEATURE_NAME_MAP.get(col, col)] = round(imp, 4)
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        feat_imp_sorted = dict(sorted(feat_imp_dict.items(), key=lambda x: x[1], reverse=True))
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.model_files["feature_importance"], "w") as f:
            json.dump(feat_imp_sorted, f, ensure_ascii=False, indent=4)
        
        # ç»˜åˆ¶è‹±æ–‡å›¾è¡¨
        plt.figure(figsize=(10, 6))
        plt.bar(feat_imp_sorted.keys(), feat_imp_sorted.values(), color='#3498db')
        plt.title("Core Factors Affecting Project Success/Failure (Feature Importance)", fontsize=12)
        plt.xlabel("Influencing Factors", fontsize=10)
        plt.ylabel("Importance Value", fontsize=10)
        plt.xticks(rotation=45, ha='right')  
        plt.tight_layout()  
        # ä¿å­˜å›¾ç‰‡åˆ°model_outputæ–‡ä»¶å¤¹
        plt.savefig(os.path.join(self.model_dir, "feature_importance_analysis.png"), dpi=300)
        plt.close()
        
        return feat_imp_sorted
        # -------------------------- åˆ†æå¤±è´¥å½±å“å› ç´  --------------------------
    def _analyze_failure_factors(self, df_train, feat_imp_sorted):
        """
        ç»“åˆç‰¹å¾é‡è¦æ€§å’Œè®­ç»ƒæ•°æ®ï¼Œåˆ†æâ€œå“ªäº›å› ç´ å¯¼è‡´é¡¹ç›®å¤±è´¥â€ï¼ˆè‹±æ–‡ç»“è®ºï¼‰
        :param df_train: è®­ç»ƒæ•°æ®é›†
        :param feat_imp_sorted: æ’åºåçš„ç‰¹å¾é‡è¦æ€§å­—å…¸
        :return: å¤±è´¥å½±å“å› ç´ çš„è‹±æ–‡ç»“è®º
        """
        failure_factors = []
        # 1. åˆ†æç­¹æ¬¾ç›®æ ‡çš„å½±å“
        top_feat = list(feat_imp_sorted.keys())[0]
        if top_feat == "Funding Goal (USD)":
            # è®¡ç®—å¤±è´¥/æˆåŠŸé¡¹ç›®çš„ç­¹æ¬¾ç›®æ ‡å‡å€¼
            fail_goal_mean = df_train[df_train["label"] == 0]["goal_usd"].mean()
            succ_goal_mean = df_train[df_train["label"] == 1]["goal_usd"].mean()
            failure_factors.append(f"âœ… Core Failure Factor: Excessively high funding goal (Failed projects: ${round(fail_goal_mean, 2)}, Successful projects: ${round(succ_goal_mean, 2)})")
        
        # 2. åˆ†æä¼—ç­¹å‘¨æœŸçš„å½±å“
        if "Funding Duration (days)" in feat_imp_sorted:
            fail_duration_mean = df_train[df_train["label"] == 0]["duration_days"].mean()
            succ_duration_mean = df_train[df_train["label"] == 1]["duration_days"].mean()
            failure_factors.append(f"âœ… Important Failure Factor: Unreasonable funding duration (Failed projects: {round(fail_duration_mean, 1)} days, Successful projects: {round(succ_duration_mean, 1)} days)")
        
        # 3. åˆ†æåˆ†ç±»/å›½å®¶çš„å½±å“
        if "Project Category" in feat_imp_sorted:
            failure_factors.append(f"âœ… Important Failure Factor: Project category (Significant differences in failure rates across categories)")
        if "Country" in feat_imp_sorted:
            failure_factors.append(f"âœ… Important Failure Factor: Country (Large differences in success rates across regions)")
        
        # 4. è¡¥å……å­—æ®µç¼ºå¤±çš„å½±å“ï¼ˆéæ¨¡å‹ç‰¹å¾ï¼Œä½†ä¸šåŠ¡ä¸Šé‡è¦ï¼‰
        failure_factors.append(f"âœ… Critical Failure Factor: Missing core fields (e.g., deadline/goal missing directly reduces success rate)")
        
        return failure_factors

    # -------------------------- æ­¥éª¤1ï¼šå…¨é‡CSVè®­ç»ƒ --------------------------    
    def full_csv_train(self):
        print("ğŸ“Œ Starting training (reading all CSV files in core folder)...")
        all_csv_files = [f for f in os.listdir(CORE_FOLDER) if f.endswith(".csv")]
        if not all_csv_files:
            raise ValueError(f"âŒ No CSV files found in core folder: {CORE_FOLDER}!")
        
        df_list = []
        for file in all_csv_files:
            file_path = os.path.join(CORE_FOLDER, file)
            try:
                df = pd.read_csv(file_path, encoding="utf-8")
            except:
                df = pd.read_csv(file_path, encoding="gbk")
            df_list.append(df)
            print(f"âœ… Loaded training file: {file} (Records: {len(df)})")
        df_train = pd.concat(df_list, ignore_index=True)
        print(f"\nğŸ“Š Total training data after merging: {len(df_train)} records\n")

        df_train = df_train[df_train["state"].str.lower().isin(["successful", "failed"])].copy()
        df_train["label"] = df_train["state"].map({"successful": 1, "failed": 0})

        df_train["fx_rate"] = df_train["fx_rate"].fillna(1.0)
        df_train["goal"] = df_train["goal"].fillna(0)
        df_train["country"] = df_train["country"].fillna("OTHER")
        df_train["category"] = df_train["category"].fillna('{"name":"OTHER"}')

        df_train["goal_usd"] = df_train["goal"] * df_train["fx_rate"]
        df_train["duration_days"] = df_train.apply(
            lambda x: (self._safe_ts_convert(x["deadline"]) - self._safe_ts_convert(x["launched_at"])).days,
            axis=1
        )
        df_train["main_category"] = df_train["category"].apply(self._safe_parse_category)

        le_cat = LabelEncoder()
        df_train["cat_enc"] = le_cat.fit_transform(df_train["main_category"])
        le_country = LabelEncoder()
        df_train["country_enc"] = le_country.fit_transform(df_train["country"])

        # ä¿®å¤ï¼šè®­ç»ƒé˜¶æ®µå°±ç¡®ä¿ä¸­ä½æ•°â‰¥0.01ï¼Œä»æ ¹æºé¿å…é™¤ä»¥0
        goal_median = df_train.groupby("main_category")["goal_usd"].median().to_dict()
        goal_median = {k: max(v, 0.01) for k, v in goal_median.items()}
        
        df_train["goal_reasonable"] = df_train.apply(
            lambda x: x["goal_usd"] / goal_median.get(x["main_category"], 1), axis=1
        ).clip(0, 1000)
        df_train["fund_sim"] = (1 / df_train["goal_reasonable"]) * df_train["cat_enc"]
        df_train["time_ratio"] = df_train["duration_days"].apply(lambda x: max(0.01, (x-7)/max(x, 1)))

        feature_cols = ["duration_days", "cat_enc", "country_enc", "goal_usd", "fund_sim", "time_ratio"]
        with open(self.model_files["feats"], "w") as f:
            json.dump(feature_cols, f)
        with open(self.model_files["goal_median"], "w") as f:
            json.dump(goal_median, f)
        joblib.dump(le_cat, self.model_files["le_cat"])
        joblib.dump(le_country, self.model_files["le_country"])

        # è®­ç»ƒå‰æ¸…ç†å¼‚å¸¸å€¼
        X = df_train[feature_cols].fillna(0)
        X = X.replace([np.inf, -np.inf], 0)
        X = X.clip(-1e18, 1e18)
        
        y = df_train["label"]
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)  # æå‰ä¿å­˜æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
        model = RandomForestClassifier(random_state=42, n_estimators=150)
        model.fit(X_scaled, y)

        joblib.dump(model, self.model_files["model"])        
        joblib.dump(scaler, self.model_files["scaler"])
        
        # ===================== æ–°å¢ï¼šè®¡ç®—å¹¶ä¿å­˜æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡ =====================
        # ç”¨è®­ç»ƒé›†é¢„æµ‹ï¼Œè®¡ç®—å‡†ç¡®ç‡
        y_pred = model.predict(X_scaled)
        model_accuracy = accuracy_score(y, y_pred)
        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶
        accuracy_dict = {
            "overall_accuracy": round(model_accuracy, 4),
            "accuracy_percentage": round(model_accuracy * 100, 2),
            "calculation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(self.model_files["model_accuracy"], "w") as f:
            json.dump(accuracy_dict, f, ensure_ascii=False, indent=4)
        
        # ===================== æå–ç‰¹å¾é‡è¦æ€§ + åˆ†æå¤±è´¥å› ç´  =====================
        print("\nğŸ“Š Extracting feature importance (failure factors)...")
        feat_imp_sorted = self._extract_feature_importance(model, feature_cols)
        failure_factors = self._analyze_failure_factors(df_train, feat_imp_sorted)
        
        # æ‰“å°ç‰¹å¾é‡è¦æ€§å’Œå¤±è´¥å› ç´ ç»“è®º
        print("\nğŸ¯ Feature Importance Ranking (Impact on Success/Failure):")
        for idx, (feat, imp) in enumerate(feat_imp_sorted.items(), 1):
            print(f"   {idx}. {feat}: {imp}")
        
        # æ–°å¢ï¼šæ‰“å°æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡
        print(f"\nğŸ“ˆ Random Forest Model Overall Accuracy:")
        print(f"   - Training Set Accuracy: {accuracy_dict['overall_accuracy']} ({accuracy_dict['accuracy_percentage']}%)")
        
        print("\nğŸ“– Core Failure Factors Analysis:")
        for factor in failure_factors:
            print(f"   {factor}")
        
        print("\nâœ… Training completed! Model + feature importance saved to model_output folder\n")

    # -------------------------- æ­¥éª¤2ï¼šå•æ–‡ä»¶é¢„æµ‹æ ¸å¿ƒé€»è¾‘ --------------------------
    def _predict_single_csv(self, csv_path):
        model = joblib.load(self.model_files["model"])
        scaler = joblib.load(self.model_files["scaler"])
        le_cat = joblib.load(self.model_files["le_cat"])
        le_country = joblib.load(self.model_files["le_country"])
        with open(self.model_files["feats"], "r") as f:
            feature_cols = json.load(f)
        with open(self.model_files["goal_median"], "r") as f:
            goal_median = json.load(f)

        try:
            df_pred = pd.read_csv(csv_path, encoding="utf-8")
        except:
            df_pred = pd.read_csv(csv_path, encoding="gbk")
        file_name = os.path.basename(csv_path)
        print(f"ğŸ” Processing prediction file: {file_name} (Total projects: {len(df_pred)})")        
        # æ‹†åˆ†idå­—æ®µçš„å¡«å……é€»è¾‘
        fill_defaults = {
            "fx_rate": 1.0,
            "goal": 0,
            "country": "OTHER",
            "category": '{"name":"OTHER"}',
            "launched_at": 1577836800,
            "deadline": self._get_beijing_time_timestamp(),
            "name": "æœªçŸ¥é¡¹ç›®"
        }
        # 1. å¤„ç†å…¶ä»–å­—æ®µï¼ˆå…ˆå¡«å……é»˜è®¤å€¼ï¼Œå†è¯†åˆ«ç¼ºå¤±ï¼‰
        for col, val in fill_defaults.items():
            if col not in df_pred.columns:
                df_pred[col] = val
            # å…ˆæ ‡è®°åŸå§‹ç¼ºå¤±å€¼ï¼Œå†å¡«å……ï¼ˆé¿å…å¡«å……åæ— æ³•è¯†åˆ«ï¼‰
            df_pred[f"{col}_original"] = df_pred[col]
            df_pred[col] = df_pred[col].fillna(val)
        
        # 2. å•ç‹¬å¤„ç†idå­—æ®µ
        if "id" not in df_pred.columns:
            df_pred["id_original"] = np.nan
            df_pred["id"] = list(range(len(df_pred)))
        else:
            df_pred["id_original"] = df_pred["id"]
            df_pred["id"] = df_pred["id"].fillna(0)

        # è®¡ç®—æ ¸å¿ƒç‰¹å¾
        df_pred["goal_usd"] = df_pred["goal"] * df_pred["fx_rate"]
        df_pred["duration_days"] = df_pred.apply(
            lambda x: (self._safe_ts_convert(x["deadline"]) - self._safe_ts_convert(x["launched_at"])).days,
            axis=1
        )
        df_pred["main_category"] = df_pred["category"].apply(self._safe_parse_category)

        df_pred["cat_enc"] = df_pred["main_category"].apply(            
            lambda x: le_cat.transform([x])[0] if x in le_cat.classes_ else le_cat.transform(["OTHER"])[0]
        )
        df_pred["country_enc"] = df_pred["country"].apply(
            lambda x: le_country.transform([x])[0] if x in le_country.classes_ else le_country.transform(["OTHER"])[0]
        )

        # åˆ†æ¯æœ€å°è®¾ä¸º0.01ï¼Œé¿å…é™¤ä»¥0äº§ç”Ÿæ— ç©·å¤§
        df_pred["goal_reasonable"] = df_pred.apply(
            lambda x: x["goal_usd"] / max(goal_median.get(x["main_category"], 1), 0.01), axis=1
        ).clip(0, 1000)
        df_pred["fund_sim"] = (1 / df_pred["goal_reasonable"]) * df_pred["cat_enc"]
        df_pred["time_ratio"] = df_pred["duration_days"].apply(lambda x: max(0.01, (x-7)/max(x, 1)))

        # æ¸…ç†æ— ç©·å¤§/å¼‚å¸¸å€¼ï¼Œé¿å…æ¨¡å‹æŠ¥é”™
        X_pred = df_pred[feature_cols].fillna(0)
        X_pred = X_pred.replace([np.inf, -np.inf], 0)
        X_pred = X_pred.clip(-1e18, 1e18)
        
        X_scaled = scaler.transform(X_pred)
        pred_prob = model.predict_proba(X_scaled)[:, 1]
        pred_state = ["successful" if p > 0.5 else "failed" for p in pred_prob]

       
        # æ¢å¤åŸå§‹å­—æ®µç”¨äºç¼ºå¤±æ£€æµ‹
        for field in CHECK_MISSING_FIELDS.keys():
            if f"{field}_original" in df_pred.columns:
                df_pred[field] = df_pred[f"{field}_original"]
                df_pred.drop(columns=[f"{field}_original"], inplace=True)
        
        # ç”Ÿæˆç¼ºå¤±å­—æ®µè¯¦æƒ…
        df_pred["æ•°æ®ç¼ºå¤±æƒ…å†µ"] = df_pred.apply(self._get_missing_fields_detail, axis=1)

        result_df = pd.DataFrame({
            "Project ID": df_pred["id"],  
            "Project Name": df_pred["name"].apply(lambda x: str(x)[:30]+"..." if len(str(x))>30 else str(x)),            
            "Main Category": df_pred["main_category"],
            "Country": df_pred["country"],
            "Funding Goal (USD)": df_pred["goal_usd"].round(2),
            "Funding Duration (days)": df_pred["duration_days"],
            "Predicted Status": pred_state,
            "Success Probability": np.round(pred_prob, 4),
            "Missing Fields": df_pred["æ•°æ®ç¼ºå¤±æƒ…å†µ"], 
            "Source File": file_name
        })

        # å•ä¸ªé¢„æµ‹ç»“æœä¿å­˜åˆ°ã€Œå•ä¸ªæ–‡ä»¶é¢„æµ‹ç»“æœã€å­æ–‡ä»¶å¤¹
        single_result_path = os.path.join(self.pred_result_dir, f"prediction_result_{file_name}")
        result_df.to_csv(single_result_path, index=False, encoding="utf-8-sig")
        print(f"âœ… Prediction completed for {file_name}! Saved to: {single_result_path}\n")
        return result_df

    # -------------------------- æ­¥éª¤3ï¼šå…¨é‡CSVé¢„æµ‹ + å¼ºåˆ¶åˆå¹¶æ±‡æ€» --------------------------
    def full_csv_predict(self):
        print("ğŸ“Œ Starting batch prediction (reading all CSV files in core folder)...\n")
        all_csv_files = [f for f in os.listdir(CORE_FOLDER) if f.endswith(".csv")]
        if not all_csv_files:
            raise ValueError(f"âŒ No CSV files found in core folder: {CORE_FOLDER}!")
        
        # è¿‡æ»¤æ‰å·²ç”Ÿæˆçš„é¢„æµ‹ç»“æœæ–‡ä»¶ï¼Œåªå¤„ç†åŸå§‹æ•°æ®æ–‡ä»¶
        pred_files = [f for f in all_csv_files if not f.startswith("prediction_result") and not f.startswith("full_prediction_summary")]
        if not pred_files:
            print("âš ï¸ No original CSV files found for prediction (filtered result files)")
            return
        
        all_results = []
        processed_files = []
        for file in pred_files:
            file_path = os.path.join(CORE_FOLDER, file)
            try:
                result_df = self._predict_single_csv(file_path)                
                all_results.append(result_df)
                processed_files.append(file)
            except Exception as e:
                print(f"âŒ Failed to process {file}: {str(e)}, skipped")
                continue
        
        # å¼ºåˆ¶åˆå¹¶æ‰€æœ‰æˆåŠŸå¤„ç†çš„ç»“æœ
        if all_results:
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            merge_df = pd.concat(all_results, ignore_index=True)
            # æ¸…ç†åˆå¹¶åçš„å¼‚å¸¸å€¼ï¼ˆé¿å…ç©ºå€¼/ç‰¹æ®Šå­—ç¬¦ï¼‰
            merge_df = merge_df.fillna("Unknown")
            merge_df = merge_df.replace([np.inf, -np.inf], 0)
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ±‡æ€»æ–‡ä»¶ï¼ˆé¿å…è¦†ç›–ï¼‰ï¼Œæ±‡æ€»æ–‡ä»¶ä»ä¿å­˜åœ¨æ ¸å¿ƒæ–‡ä»¶å¤¹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merge_result_path = os.path.join(CORE_FOLDER, f"full_prediction_summary_{timestamp}.csv")
            
            # ä¿å­˜æ±‡æ€»æ–‡ä»¶ï¼ˆutf-8-sigå…¼å®¹Excelï¼‰
            merge_df.to_csv(merge_result_path, index=False, encoding="utf-8-sig")
            
            # ç»Ÿè®¡ç¼ºå¤±æƒ…å†µï¼ˆæ–°å¢ï¼‰
            missing_stats = merge_df["Missing Fields"].value_counts()
            
            # ===================== è¯»å–ç‰¹å¾é‡è¦æ€§å’Œæ¨¡å‹å‡†ç¡®ç‡ï¼ŒåŠ å…¥æ±‡æ€»æ—¥å¿— =====================
            print("\nğŸ¯ Core Factors Affecting Project Failure (Extracted from Model):")
            if os.path.exists(self.model_files["feature_importance"]):
                with open(self.model_files["feature_importance"], "r") as f:
                    feat_imp_sorted = json.load(f)
                for idx, (feat, imp) in enumerate(feat_imp_sorted.items(), 1):
                    print(f"   {idx}. {feat} (Importance: {imp})")
            
            # æ–°å¢ï¼šæ‰“å°æ¨¡å‹å‡†ç¡®ç‡
            if os.path.exists(self.model_files["model_accuracy"]):
                with open(self.model_files["model_accuracy"], "r") as f:
                    accuracy_dict = json.load(f)
                print(f"\nğŸ“ˆ Random Forest Model Performance:")
                print(f"   - Overall Accuracy: {accuracy_dict['overall_accuracy']} ({accuracy_dict['accuracy_percentage']}%)")
            
            print("="*60)
            print(f"âœ… Batch prediction completed!")
            print(f"ğŸ“ Single prediction results path: {self.pred_result_dir}")            
            print(f"ğŸ“ Full summary file path: {merge_result_path}")
            print(f"ğŸ“Š Summary Statistics:")
            print(f"   - Processed files: {len(processed_files)}")
            print(f"   - Total projects: {len(merge_df)}")
            print(f"   - Predicted successful: {len(merge_df[merge_df['Predicted Status']=='successful'])}")
            print(f"   - Predicted failed: {len(merge_df[merge_df['Predicted Status']=='failed'])}")
            print(f"\nğŸ“‹ Missing Fields Statistics:")
            for missing_type, count in missing_stats.head(10).items():
                print(f"   - {missing_type}: {count} records")
            print("="*60)
        else:
            print("âŒ No files processed successfully, cannot generate summary")

# ===================== ä¸»æ‰§è¡Œé€»è¾‘ =====================
if __name__ == "__main__":
    processor = KSFullCSVProcessor()
    
    try:
        # å…ˆè®­ç»ƒï¼ˆå¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼‰
        if not os.path.exists(processor.model_files["model"]):
            processor.full_csv_train()
        # æ‰§è¡Œé¢„æµ‹+æ±‡æ€»
        processor.full_csv_predict()
    except Exception as e:
        print(f"\nâŒ Program execution error: {str(e)}")


# In[1]:


import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import roc_curve, auc

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®ï¼‰ =====================
CORE_FOLDER = "/Users/shiyiwen/Desktop/ç§‘ç ”/æ•°æ®æå–ä»£ç /éšæœºæ£®æ—æå–å¤±è´¥å…ƒç´ "
PROCESSED_FILE = os.path.join(CORE_FOLDER, "full_prediction_summary_20260201_174043.csv")

# ç‰¹å¾åç§°æ˜ å°„
FEATURE_MAP = {
    "goal_ratio": "Goal vs Category Median (Ratio)",
    "time_penalty": "Time Penalty (Exponential)",
    "category_risk": "Category Failure Rate",
    "combined_risk": "Combined Risk (Goal x Time)",
    "country_factor": "Country Risk Factor",
    "urgency_score": "Urgency Score (7d Ratio)"
}

# æ‰©å±•æœ‰æ•ˆçŠ¶æ€ï¼ˆåŒ…å«å–æ¶ˆ/æš‚åœï¼Œæå‡æ•°æ®é‡ï¼‰
EXTENDED_VALID_STATES = ['failed', 'successful', 'Failed', 'Successful',
                         'canceled', 'Canceled', 'suspended', 'Suspended']

# é£é™©æ ‡ç­¾æ˜ å°„ï¼ˆå–æ¶ˆ/æš‚åœè§†ä¸ºé«˜é£é™©ï¼‰
RISK_LABEL_MAP = {
    'failed': 1, 'Failed': 1,
    'canceled': 1, 'Canceled': 1,
    'suspended': 1, 'Suspended': 1,
    'successful': 0, 'Successful': 0
}

# ===================== ç»˜å›¾é…ç½® =====================
plt.rcParams.update(plt.rcParamsDefault)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ===================== æ ¸å¿ƒåˆ†æç±» =====================
class ROCBasedRiskAnalyzer:
    def __init__(self):
        self.processed_file = PROCESSED_FILE
        self.df = None
        self.raw_df = None  # ä¿å­˜åŸå§‹å…¨é‡æ•°æ®
        self.rules = {}     # ROCæœ€ä¼˜é˜ˆå€¼
        self.auc_results = {}  # ç‰¹å¾AUCå€¼

    # -------------------------- åŠ è½½é¢„å¤„ç†æ–‡ä»¶ --------------------------
    def load_processed_data(self):
        print(f"ğŸ“¥ åŠ è½½é¢„å¤„ç†æ–‡ä»¶: {self.processed_file}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.processed_file):
            raise FileNotFoundError(
                f"âŒ é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼è·¯å¾„: {self.processed_file}\n"
                f"   æç¤ºï¼šéœ€å…ˆè¿è¡Œè€ä»£ç ç”Ÿæˆ full_prediction_summary_20260201_174043.csv"
            )
        
        # è¯»å–æ–‡ä»¶
        try:
            self.raw_df = pd.read_csv(self.processed_file, encoding="utf-8-sig")
        except Exception as e:
            raise ValueError(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡
        print(f"ğŸ“ˆ åŸå§‹æ–‡ä»¶æ€»é¡¹ç›®æ•°: {len(self.raw_df)}")
        print(f"ğŸ“‹ åŸå§‹æ•°æ®çŠ¶æ€åˆ†å¸ƒ:")
        # ä»…ä¿®æ”¹ï¼šå°†stateæ”¹ä¸ºPredicted Status
        state_counts = self.raw_df['Predicted Status'].value_counts(dropna=False)
        for state, count in state_counts.items():
            print(f"   - {state}: {count} æ¡")
        
        # ç­›é€‰æ‰©å±•æœ‰æ•ˆçŠ¶æ€
        # ä»…ä¿®æ”¹ï¼šå°†stateæ”¹ä¸ºPredicted Status
        self.df = self.raw_df[self.raw_df['Predicted Status'].isin(EXTENDED_VALID_STATES)].copy()
        
        # æ„å»ºé£é™©æ ‡ç­¾
        # ä»…ä¿®æ”¹ï¼šå°†stateæ”¹ä¸ºPredicted Status
        self.df['label'] = self.df['Predicted Status'].map(RISK_LABEL_MAP)
        self.df = self.df.dropna(subset=['label'])
        self.df['label'] = self.df['label'].astype(int)
        
        # æ£€æŸ¥æ ¸å¿ƒå­—æ®µ
        required_cols = ['goal_usd', 'duration_days', 'main_category', 'country']
        # ä»…æ–°å¢ï¼šæ˜ å°„å®é™…åˆ—ååˆ°ä»£ç é¢„æœŸå­—æ®µå
        col_mapping = {
            'Funding Goal (USD)': 'goal_usd',
            'Funding Duration (days)': 'duration_days',
            'Main Category': 'main_category',
            'Country': 'country'
        }
        for old_col, new_col in col_mapping.items():
            if old_col in self.df.columns and new_col not in self.df.columns:
                self.df.rename(columns={old_col: new_col}, inplace=True)
        # æ£€æŸ¥æ ¸å¿ƒå­—æ®µ
        missing_cols = [col for col in required_cols if col not in self.df.columns]
        if missing_cols:
            raise ValueError(f"âŒ ç¼ºå°‘å…³é”®å­—æ®µ: {missing_cols}")
        
        # æ•°æ®æ¸…ç†
        self.df['duration'] = self.df['duration_days'].clip(1, 180)
        self.df['goal_usd'] = self.df['goal_usd'].clip(0, 1e6)
        
        print(f"âœ… é¢„å¤„ç†æ–‡ä»¶åŠ è½½å®Œæˆï¼")
        print(f"   ğŸ“Š æ‰©å±•åæœ‰æ•ˆé¡¹ç›®æ•°: {len(self.df)} ")
        print(f"   ğŸ“Œ é«˜é£é™©é¡¹ç›®ï¼ˆå¤±è´¥/å–æ¶ˆ/æš‚åœï¼‰: {len(self.df[self.df['label']==1])}")
        print(f"   ğŸ“Œ ä½é£é™©é¡¹ç›®ï¼ˆæˆåŠŸï¼‰: {len(self.df[self.df['label']==0])}")
        print(f"   ğŸ“Œ å“ç±»æ•°: {self.df['main_category'].nunique()}, å›½å®¶æ•°: {self.df['country'].nunique()}")

    # -------------------------- æ„å»ºå‡½æ•°ç‰¹å¾ --------------------------
    def build_functional_features(self):
        print("\nğŸ”§ æ„å»ºå‡½æ•°å…³ç³»ç‰¹å¾...")
        
        # A. ç›®æ ‡å€ç‡ï¼ˆé¡¹ç›®ç›®æ ‡/åŒå“ç±»ä¸­ä½æ•°ï¼‰
        cat_medians = self.df.groupby('main_category')['goal_usd'].median()
        self.df['goal_ratio'] = self.df.apply(
            lambda x: x['goal_usd'] / max(cat_medians.get(x['main_category'], 1), 0.01),
            axis=1
        ).clip(0, 50)

        # B. æ—¶é—´æƒ©ç½šæŒ‡æ•°ï¼ˆe^(å‘¨æœŸ/30)-1ï¼‰
        self.df['time_penalty'] = np.exp(self.df['duration'] / 30) - 1

        # C. å“ç±»é£é™©ç³»æ•°ï¼ˆå“ç±»é«˜é£é™©ç‡ï¼‰
        cat_fail_rates = self.df.groupby('main_category')['label'].mean()
        self.df['category_risk'] = self.df['main_category'].map(cat_fail_rates)

        # D. ç»„åˆé£é™©ï¼ˆç›®æ ‡å€ç‡Ã—æ—¶é—´æƒ©ç½šï¼‰
        self.df['combined_risk'] = self.df['goal_ratio'] * self.df['time_penalty']

        # E. å›½å®¶é£é™©å› å­ï¼ˆå›½å®¶é«˜é£é™©ç‡ï¼‰
        country_fail_rates = self.df.groupby('country')['label'].mean()
        self.df['country_factor'] = self.df['country'].map(country_fail_rates)

        # F. ç´§è¿«æ„Ÿåˆ†æ•°ï¼ˆ7/å‘¨æœŸï¼‰
        self.df['urgency_score'] = 7 / self.df['duration']

        # éªŒè¯ç‰¹å¾
        built_features = ['goal_ratio', 'time_penalty', 'category_risk', 
                          'combined_risk', 'country_factor', 'urgency_score']
        for feat in built_features:
            if feat not in self.df.columns:
                raise ValueError(f"âŒ ç‰¹å¾ {feat} æ„å»ºå¤±è´¥")
        print(f"âœ… 6ä¸ªå‡½æ•°ç‰¹å¾æ„å»ºå®Œæˆï¼")

    # -------------------------- ROC+AUCæ‰¾æœ€ä¼˜é˜ˆå€¼ --------------------------
    def _get_optimal_threshold(self, feature):
        y_true = self.df['label']
        y_score = self.df[feature]
        
        # è®¡ç®—ROCå‚æ•°
        fpr, tpr, thresholds = roc_curve(y_true, y_score)
        auc_value = auc(fpr, tpr)
        
        # çº¦ç™»æŒ‡æ•°æ‰¾æœ€ä¼˜é˜ˆå€¼
        j_scores = tpr - fpr
        best_idx = j_scores.argmax()
        best_threshold = thresholds[best_idx]
        best_tpr = tpr[best_idx]
        best_fpr = fpr[best_idx]
        
        return best_threshold, auc_value, best_tpr, best_fpr

    # -------------------------- å¯è§†åŒ–+è§„åˆ™æå– --------------------------
    def analyze_and_extract_rules(self):
        print("\nğŸ“Š ç”¨ROC+AUCåˆ†æé£é™©è§„åˆ™...")
        features = ['goal_ratio', 'time_penalty', 'category_risk', 
                    'combined_risk', 'country_factor', 'urgency_score']
        
        # 1. ç»˜åˆ¶ROCæ±‡æ€»å›¾
        plt.figure(figsize=(10, 8))
        for feat in features:
            y_true = self.df['label']
            y_score = self.df[feat]
            fpr, tpr, _ = roc_curve(y_true, y_score)
            auc_val = auc(fpr, tpr)
            plt.plot(fpr, tpr, lw=2, label=f'{FEATURE_MAP[feat]} (AUC={auc_val:.3f})')
        
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess (AUC=0.5)')
        plt.xlabel('False Positive Rate (FPR) - è¯¯åˆ¤ç‡', fontsize=11)
        plt.ylabel('True Positive Rate (TPR) - è¯†åˆ«ç‡', fontsize=11)
        plt.title('ROC Curves for All Risk Features', fontsize=12)
        plt.legend(loc='lower right', fontsize=10)
        plt.grid(alpha=0.3)
        roc_path = os.path.join(CORE_FOLDER, "roc_curve_summary.png")
        plt.savefig(roc_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… ROCæ±‡æ€»å›¾å·²ä¿å­˜: {roc_path}")

        # 2. ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒ+æå–è§„åˆ™
        plt.figure(figsize=(18, 10))
        for i, feat in enumerate(features):
            best_thresh, auc_val, tpr, fpr = self._get_optimal_threshold(feat)
            self.rules[feat] = best_thresh
            self.auc_results[feat] = auc_val
            
            # ç»˜åˆ¶é«˜é£é™©é¡¹ç›®åˆ†å¸ƒ
            high_risk_df = self.df[self.df['label'] == 1]
            plt.subplot(2, 3, i+1)
            sns.histplot(high_risk_df[feat], kde=True, color='#e74c3c', bins=30, alpha=0.8)
            plt.axvline(best_thresh, color='#2c3e50', linestyle='--', linewidth=2,
                        label=f'Optimal Threshold: {best_thresh:.2f}\nAUC: {auc_val:.3f}\nTPR: {tpr:.2f}, FPR: {fpr:.2f}')
            plt.title(f"High-Risk Projects: {FEATURE_MAP[feat]}", fontsize=12)
            plt.xlabel(FEATURE_MAP[feat], fontsize=10)
            plt.ylabel("Number of High-Risk Projects", fontsize=10)
            plt.legend(fontsize=9)
            plt.grid(alpha=0.3)

            # æ‰“å°è§„åˆ™è¯¦æƒ…
            self._print_rule_detail(feat, best_thresh, auc_val, tpr, fpr)

        # ä¿å­˜ç‰¹å¾åˆ†å¸ƒå›¾
        dist_path = os.path.join(CORE_FOLDER, "risk_feature_distribution.png")
        plt.tight_layout()
        plt.savefig(dist_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜: {dist_path}")

        # æ‰“å°AUCæ±‡æ€»
        print("\nğŸ“ˆ å„ç‰¹å¾AUCå€¼æ±‡æ€»:")
        print("   " + "-"*60)
        print(f"   {'ç‰¹å¾':<25} {'AUCå€¼':<10} {'ç­‰çº§':<10}")
        print("   " + "-"*60)
        for feat in features:
            auc_val = self.auc_results[feat]
            level = "ä¼˜ç§€" if auc_val >= 0.8 else "è‰¯å¥½" if auc_val >= 0.7 else "ä¸€èˆ¬"
            print(f"   {FEATURE_MAP[feat]:<25} {auc_val:.3f}      {level}")
        print("   " + "-"*60)

    def _print_rule_detail(self, feat, thresh, auc_val, tpr, fpr):
        """æ‰“å°å•ä¸ªç‰¹å¾çš„è§„åˆ™è¯¦æƒ…"""
        print(f"\n--- {FEATURE_MAP[feat]} ---")
        print(f"   ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {thresh:.2f}")
        print(f"   ğŸ“Š æ¨¡å‹æ€§èƒ½: AUC={auc_val:.3f}, è¯†åˆ«ç‡={tpr:.2f}, è¯¯åˆ¤ç‡={fpr:.2f}")
        print(f"   ğŸ”´ é£é™©è§„åˆ™: ", end="")
        
        if feat == 'goal_ratio':
            print(f"å½“ (é¡¹ç›®ç›®æ ‡ / åŒå“ç±»ä¸­ä½æ•°) > {thresh:.2f} å€æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")
        elif feat == 'time_penalty':
            days = np.log(thresh + 1) * 30
            print(f"å½“æ—¶é—´æƒ©ç½šæŒ‡æ•° > {thresh:.2f}ï¼ˆçº¦{days:.0f}å¤©ï¼‰æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")
        elif feat == 'category_risk':
            print(f"å½“å“ç±»å†å²é«˜é£é™©ç‡ > {thresh:.2%} æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")
        elif feat == 'combined_risk':
            print(f"å½“ (ç›®æ ‡å€ç‡ Ã— æ—¶é—´æƒ©ç½šæŒ‡æ•°) > {thresh:.2f} æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")
        elif feat == 'country_factor':
            print(f"å½“å›½å®¶å†å²é«˜é£é™©ç‡ > {thresh:.2%} æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")
        elif feat == 'urgency_score':
            days = 7 / thresh
            print(f"å½“ç´§è¿«æ„Ÿåˆ†æ•° > {thresh:.2f}ï¼ˆçº¦{days:.0f}å¤©ï¼‰æ—¶ï¼Œåˆ¤å®šä¸ºé«˜é£é™©")

    # -------------------------- å¯¼å‡ºé£é™©è§„åˆ™ä»£ç ï¼ˆä¿®å¤ç¼©è¿›+å˜é‡ï¼‰ --------------------------
    def export_risk_rules(self):
        print("\nğŸ’¾ å¯¼å‡ºé£é™©è§„åˆ™ä»£ç ...")
        
        # è®¡ç®—åŸºå‡†æ•°æ®
        cat_medians = self.df.groupby('main_category')['goal_usd'].median().to_dict()
        cat_risk_rates = self.df.groupby('main_category')['label'].mean().to_dict()
        
        # ç”Ÿæˆè§„åˆ™ä»£ç ï¼ˆç»Ÿä¸€ç¼©è¿›ï¼Œé¿å…æ ¼å¼é”™è¯¯ï¼‰
        rule_code = f"""import numpy as np

# ===================== ä¼—ç­¹é¡¹ç›®å¤±è´¥é£é™©è§„åˆ™ï¼ˆROC+AUCä¼˜åŒ–ï¼‰ =====================
# æ•°æ®æ¥æºï¼š{PROCESSED_FILE}
# æ‰©å±•ï¼šå¤±è´¥/å–æ¶ˆ/æš‚åœ=é«˜é£é™©ï¼ŒæˆåŠŸ=ä½é£é™©

# åŸºå‡†æ•°æ®
CATEGORY_MEDIAN_GOAL = {cat_medians}
CATEGORY_RISK_RATE = {cat_risk_rates}
OPTIMAL_THRESHOLDS = {self.rules}
FEATURE_AUC = {self.auc_results}

def judge_project_risk(project_data):
    risk_reasons = []
    is_high_risk = False

    # 1. ç›®æ ‡å€ç‡é£é™©
    median_goal = CATEGORY_MEDIAN_GOAL.get(project_data['main_category'], 5000)
    goal_ratio_val = project_data['goal_usd'] / max(median_goal, 0.01)
    if goal_ratio_val > OPTIMAL_THRESHOLDS['goal_ratio']:
        risk_reasons.append(f"ç›®æ ‡å€ç‡è¿‡é«˜ï¼ˆ{{goal_ratio_val:.2f}} > é˜ˆå€¼{{OPTIMAL_THRESHOLDS['goal_ratio']:.2f}}ï¼‰")
        is_high_risk = True

    # 2. å‘¨æœŸé£é™©
    time_penalty_val = np.exp(project_data['duration_days'] / 30) - 1
    if time_penalty_val > OPTIMAL_THRESHOLDS['time_penalty']:
        risk_reasons.append(f"å‘¨æœŸé£é™©è¿‡é«˜ï¼ˆæƒ©ç½šæŒ‡æ•°{{time_penalty_val:.2f}} > é˜ˆå€¼{{OPTIMAL_THRESHOLDS['time_penalty']:.2f}}ï¼‰")
        is_high_risk = True

    # 3. ç»„åˆé£é™©
    combined_risk_val = goal_ratio_val * time_penalty_val
    if combined_risk_val > OPTIMAL_THRESHOLDS['combined_risk']:
        risk_reasons.append(f"ç»„åˆé£é™©è¿‡é«˜ï¼ˆ{{combined_risk_val:.2f}} > é˜ˆå€¼{{OPTIMAL_THRESHOLDS['combined_risk']:.2f}}ï¼‰")
        is_high_risk = True

    # 4. å“ç±»é£é™©
    cat_risk_val = CATEGORY_RISK_RATE.get(project_data['main_category'], 0.5)
    if cat_risk_val > OPTIMAL_THRESHOLDS['category_risk']:
        risk_reasons.append(f"å“ç±»é£é™©è¿‡é«˜ï¼ˆé«˜é£é™©ç‡{{cat_risk_val:.2%}} > é˜ˆå€¼{{OPTIMAL_THRESHOLDS['category_risk']:.2%}}ï¼‰")
        is_high_risk = True

    if not risk_reasons:
        risk_reasons.append("æ— é«˜é£é™©å› ç´ ï¼Œé£é™©å¯æ§")
    return is_high_risk, risk_reasons

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    test_project = {{
        'goal_usd': 15000,
        'duration_days': 60,
        'main_category': 'Technology',
        'country': 'US'
    }}
    risk_result, reasons = judge_project_risk(test_project)
    print(f"é¡¹ç›®é£é™©è¯„ä¼°ï¼š{{'é«˜é£é™©' if risk_result else 'ä½é£é™©'}}")
    print("é£é™©åŸå› ï¼š")
    for i, reason in enumerate(reasons, 1):
        print(f"  {{i}}. {{reason}}")
        """
        
        # ä¿å­˜è§„åˆ™æ–‡ä»¶
        rule_path = os.path.join(CORE_FOLDER, "crowdfunding_risk_rules.py")
        with open(rule_path, "w", encoding="utf-8") as f:
            f.write(rule_code)
        
        print(f"âœ… è§„åˆ™ä»£ç å·²ä¿å­˜è‡³: {rule_path}")
        print(f"   ğŸ“Œ ä½¿ç”¨ï¼šå¯¼å…¥ judge_project_risk å‡½æ•°å³å¯åˆ¤æ–­é£é™©")

    # -------------------------- è¿è¡Œå…¨æµç¨‹ --------------------------
    def run_full_analysis(self):
        try:
            self.load_processed_data()
            self.build_functional_features()
            self.analyze_and_extract_rules()
            self.export_risk_rules()
            print("\nğŸ‰ é£é™©è§„åˆ™åˆ†æå…¨æµç¨‹å®Œæˆï¼")
        except Exception as e:
            print(f"\nâŒ åˆ†æå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

# ===================== ä¸»æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    analyzer = ROCBasedRiskAnalyzer()
    analyzer.run_full_analysis()

